defaults:
  - _self_
  - dataset: glue
  - task: cola
  - model: deberta
  - hydra: default.yaml

# Root directory for dataset, model outputs, and logs
root_dir: "datasets"

# Dataloader configurations
batch_size: 8
num_workers: 2  # Number of workers for data loading

# Hyperparameters
num_epochs: 10
eval_steps: 1000
learning_rate: 5e-5
weight_decay: 1e-4
scheduler:  # MultiStepLR
  milestones: [6, 8]
  gamma: 0.1

# Lora configurations
apply_lora: False
lora_r: 16
lora_alpha: 32

# Wandb configurations
wandb:
  project: "deberta-glue"
  entity: "eddie880509"
  tags: ["lora"]

# GPU ID
gpu: 0
