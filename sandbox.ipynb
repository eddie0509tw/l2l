{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from lightning.pytorch import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from dats import build_dataset\n",
    "from model import build_model\n",
    "# from tools.vis import save_attention_loc\n",
    "from tools.validator import build_validator\n",
    "\n",
    "@hydra.main(config_path=\"conf\", config_name=\"finetune\", version_base=\"1.3\")\n",
    "def run(cfg: DictConfig):\n",
    "    save_dir = (f\"{cfg.dataset.name}_result/{cfg.task.name}/{cfg.model.name}\")\n",
    "    save_path = os.path.join(cfg.root_dir, save_dir)\n",
    "    meta_dataloader = build_dataset(cfg)\n",
    "    train_loader = meta_dataloader['train']\n",
    "    val_loader = meta_dataloader['val']\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    ckpt_cb = ModelCheckpoint(\n",
    "        dirpath=os.path.join(save_path, \"weight\"),\n",
    "        filename=\"best\",\n",
    "        monitor='val/loss',\n",
    "        mode='min',\n",
    "        save_top_k=1,\n",
    "        save_last=True)\n",
    "    callbacks = [lr_monitor, ckpt_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir: datasets\n",
      "batch_size: 8\n",
      "num_workers: 2\n",
      "num_epochs: 10\n",
      "eval_steps: 1000\n",
      "learning_rate: 5.0e-05\n",
      "weight_decay: 0.0001\n",
      "scheduler:\n",
      "  milestones:\n",
      "  - 6\n",
      "  - 8\n",
      "  gamma: 0.1\n",
      "apply_lora: false\n",
      "lora_r: 16\n",
      "lora_alpha: 32\n",
      "wandb:\n",
      "  project: deberta-glue\n",
      "  entity: eddie880509\n",
      "  tags:\n",
      "  - lora\n",
      "gpu: 0\n",
      "load_pretrained: true\n",
      "dataset:\n",
      "  name: glue\n",
      "  root_dir: datasets\n",
      "  overwrite_cache: false\n",
      "  use_tokenizer: true\n",
      "task:\n",
      "  name: cola\n",
      "  max_seq_length: 64\n",
      "  num_labels: 2\n",
      "  learning_rate: 5.0e-05\n",
      "model:\n",
      "  name: deberta_base\n",
      "  model_name: microsoft/deberta-base\n",
      "  use_hf_weights: true\n",
      "  output_attentions: true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172574/771495399.py:7: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"conf\")  # Make sure 'conf' is the correct path relative to your notebook\n"
     ]
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose, initialize\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "# Initialize Hydra and compose the config manually\n",
    "initialize(config_path=\"conf\")  # Make sure 'conf' is the correct path relative to your notebook\n",
    "cfg = compose(config_name=\"finetune\")  # Load the \"train.yaml\" config\n",
    "\n",
    "# Print the default configuration\n",
    "print(OmegaConf.to_yaml(cfg))  # Optional: To see the loaded configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "DebertaForSequenceClassification(\n",
      "  (deberta): DebertaModel(\n",
      "    (embeddings): DebertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
      "      (LayerNorm): DebertaLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): DebertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaLayer(\n",
      "          (attention): DebertaAttention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
      "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): DebertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): DebertaLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): DebertaLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(1024, 768)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import build_model\n",
    "model = build_model(cfg)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "ways = 5\n",
    "model_name='google/vit-base-patch16-224-in21k'\n",
    "config = ViTConfig.from_pretrained(model_name, num_labels=ways)\n",
    "model = ViTForImageClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): ViTForImageClassification(\n",
      "      (vit): ViTModel(\n",
      "        (embeddings): ViTEmbeddings(\n",
      "          (patch_embeddings): ViTPatchEmbeddings(\n",
      "            (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (encoder): ViTEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x ViTLayer(\n",
      "              (attention): ViTSdpaAttention(\n",
      "                (attention): ViTSdpaSelfAttention(\n",
      "                  (query): Linear(\n",
      "                    in_features=768, out_features=768, bias=True\n",
      "                    (lora_route): Linear(in_features=768, out_features=1, bias=False)\n",
      "                    (lora_A0): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    (lora_B0): Linear(in_features=16, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(\n",
      "                    in_features=768, out_features=768, bias=True\n",
      "                    (lora_route): Linear(in_features=768, out_features=1, bias=False)\n",
      "                    (lora_A0): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    (lora_B0): Linear(in_features=16, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): ViTSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): ViTIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): ViTOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"query\",\"value\"],\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        modules_to_save=[\"classifier\"],\n",
    "        lora_nums=1,\n",
    "    )\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dats.dataset import create_miniimgnat, create_omniglot\n",
    "ways = 5\n",
    "shots = 5\n",
    "tasksets = create_miniimgnat(train_samples=2*shots,\n",
    "                                    train_ways=ways,\n",
    "                                    test_samples=2*shots,\n",
    "                                    test_ways=ways,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.vit.encoder.layer.0.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.classifier.weight torch.Size([5, 768])\n",
      "base_model.model.classifier.bias torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ViTForImageClassification(\n",
       "      (vit): ViTModel(\n",
       "        (embeddings): ViTEmbeddings(\n",
       "          (patch_embeddings): ViTPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): ViTEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x ViTLayer(\n",
       "              (attention): ViTSdpaAttention(\n",
       "                (attention): ViTSdpaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=768, out_features=768, bias=True\n",
       "                    (lora_route): Linear(in_features=768, out_features=1, bias=False)\n",
       "                    (lora_A0): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    (lora_B0): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=768, out_features=768, bias=True\n",
       "                    (lora_route): Linear(in_features=768, out_features=1, bias=False)\n",
       "                    (lora_A0): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    (lora_B0): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): ViTSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): ViTIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): ViTOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "cuda = torch.cuda.is_available()\n",
    "seed = 42\n",
    "if cuda and torch.cuda.device_count():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cloning - module.base_model.model.vit.embeddings.cls_token: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.embeddings.position_embeddings: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.embeddings.patch_embeddings.projection.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.embeddings.patch_embeddings.projection.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.layernorm.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.layernorm.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.classifier.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.classifier.bias: requires_grad=True, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.embeddings.cls_token: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.embeddings.position_embeddings: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.embeddings.patch_embeddings.projection.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.embeddings.patch_embeddings.projection.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f899d453f70>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.layernorm.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.layernorm.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.classifier.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "After cloning - module.base_model.model.classifier.bias: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f894c968520>\n",
      "-------------------\n",
      "(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-1.3400e-06, -1.5788e-06, -4.2242e-06,  ..., -3.0777e-06,\n",
      "         -1.6576e-06,  6.7476e-06],\n",
      "        [-2.8514e-06,  1.4013e-06, -1.8485e-06,  ..., -3.4677e-06,\n",
      "          1.7336e-06, -8.3766e-07],\n",
      "        [-5.9732e-06,  7.5161e-06, -1.5733e-06,  ..., -9.7243e-06,\n",
      "         -1.2129e-06, -1.2045e-06],\n",
      "        ...,\n",
      "        [ 6.2062e-06, -4.1419e-06, -2.4720e-06,  ...,  9.4650e-07,\n",
      "          1.8808e-06, -1.4625e-06],\n",
      "        [ 6.5303e-06, -1.4497e-06, -7.7837e-07,  ..., -1.5248e-06,\n",
      "          6.6240e-07,  1.7087e-07],\n",
      "        [ 5.6366e-06, -6.1746e-07,  4.5184e-07,  ...,  2.4607e-06,\n",
      "          2.2091e-06, -1.1889e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-8.5963e-06, -9.6918e-06,  8.2210e-06,  ...,  3.6507e-05,\n",
      "          4.0615e-05, -3.3025e-05],\n",
      "        [ 1.5393e-04,  1.7929e-04, -5.8855e-05,  ...,  3.5741e-05,\n",
      "         -8.5687e-05, -1.1383e-05],\n",
      "        [-5.2550e-05, -5.4797e-05, -1.6964e-05,  ...,  1.2303e-05,\n",
      "          7.8765e-05, -2.3886e-05],\n",
      "        ...,\n",
      "        [ 3.0123e-06,  1.3860e-06,  8.4207e-06,  ..., -5.7641e-06,\n",
      "          2.4973e-06,  7.2174e-06],\n",
      "        [ 2.7588e-07, -9.8213e-06,  9.8365e-06,  ..., -7.7411e-08,\n",
      "         -1.1235e-07,  1.2702e-05],\n",
      "        [-5.8624e-06, -7.1806e-06, -1.7650e-05,  ...,  2.1298e-05,\n",
      "          5.3847e-06, -2.0583e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.5665e-05, -2.1997e-05, -1.9680e-05,  ...,  3.6071e-05,\n",
      "         -4.7810e-06, -2.6878e-05],\n",
      "        [ 2.4564e-05, -2.5598e-05,  2.3754e-06,  ...,  3.8204e-05,\n",
      "         -1.3240e-05, -2.1959e-05],\n",
      "        [-7.6261e-06, -1.7646e-05,  5.2549e-06,  ...,  1.4519e-05,\n",
      "         -1.7379e-05, -2.3411e-05],\n",
      "        ...,\n",
      "        [-6.7448e-06, -2.5113e-06,  1.4872e-06,  ..., -4.8397e-06,\n",
      "         -2.5166e-06,  1.1524e-06],\n",
      "        [ 2.7113e-06, -8.0297e-07,  6.7786e-06,  ..., -1.2285e-06,\n",
      "          8.1595e-07,  1.2309e-06],\n",
      "        [-8.5641e-06,  5.7893e-06,  5.1175e-06,  ..., -7.7170e-06,\n",
      "          8.6555e-06,  5.6638e-07]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 3.8810e-07,  7.9346e-05,  5.9436e-06,  ...,  2.8091e-05,\n",
      "          5.6942e-05, -7.9561e-06],\n",
      "        [ 1.0406e-05,  7.7337e-06, -2.2418e-06,  ..., -1.0584e-05,\n",
      "         -1.0235e-05,  1.9939e-05],\n",
      "        [ 8.4903e-05, -6.6069e-05, -2.6482e-06,  ...,  8.0601e-06,\n",
      "         -1.6745e-04,  5.1427e-05],\n",
      "        ...,\n",
      "        [-3.4909e-05, -5.2131e-05,  9.8972e-05,  ...,  2.8736e-05,\n",
      "          6.4558e-05, -1.3833e-05],\n",
      "        [ 8.7048e-07, -4.0486e-06,  2.6357e-05,  ..., -3.7114e-05,\n",
      "         -1.1709e-04, -2.4912e-05],\n",
      "        [ 4.2103e-05, -1.4039e-05,  2.5523e-05,  ..., -1.1802e-05,\n",
      "          5.0013e-05, -2.4649e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-7.3233e-06,  1.0061e-05,  2.5695e-06,  ..., -2.2371e-05,\n",
      "          8.4974e-06, -3.5045e-06],\n",
      "        [ 4.3965e-06, -1.5084e-05, -1.9881e-05,  ...,  3.7575e-06,\n",
      "          1.4080e-05, -1.2423e-05],\n",
      "        [ 1.1974e-05, -1.7210e-05, -8.1778e-06,  ...,  6.2894e-06,\n",
      "          7.0714e-06, -2.3268e-05],\n",
      "        ...,\n",
      "        [ 6.3857e-06, -3.3847e-05, -3.2363e-05,  ...,  1.6044e-05,\n",
      "          4.5310e-05, -1.0441e-05],\n",
      "        [ 1.4878e-05,  6.9773e-06, -9.0118e-06,  ..., -1.4893e-05,\n",
      "          4.9715e-07, -3.6424e-05],\n",
      "        [-1.1266e-05,  1.4813e-05,  6.7571e-06,  ...,  1.0645e-05,\n",
      "         -4.2619e-05, -7.2546e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-5.0149e-05,  6.1353e-05,  1.7288e-05,  ..., -4.7309e-06,\n",
      "          5.6433e-05,  9.0029e-05],\n",
      "        [ 1.2037e-04, -5.9926e-05,  7.2783e-05,  ...,  2.4732e-05,\n",
      "         -4.8971e-05,  8.3568e-06],\n",
      "        [-1.8672e-05, -2.7321e-04, -1.8832e-05,  ..., -1.4211e-05,\n",
      "          5.1346e-05, -8.7825e-06],\n",
      "        ...,\n",
      "        [-5.9809e-05,  8.9081e-05,  6.0867e-05,  ..., -4.7980e-06,\n",
      "         -3.3022e-05,  6.6086e-05],\n",
      "        [-4.8624e-05,  1.5393e-04,  8.8733e-05,  ...,  5.9324e-05,\n",
      "         -4.0905e-05, -8.9497e-05],\n",
      "        [ 6.4857e-05, -4.5521e-05, -8.7163e-05,  ...,  1.4956e-06,\n",
      "          3.8769e-06,  1.6179e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.9419e-06, -3.4890e-05,  5.6110e-06,  ...,  2.0983e-05,\n",
      "          3.5313e-07, -9.5442e-06],\n",
      "        [ 2.0121e-06,  3.6006e-05,  1.1740e-06,  ...,  3.7010e-06,\n",
      "         -6.3783e-06,  1.1272e-05],\n",
      "        [-7.9313e-06, -2.1144e-06, -2.6891e-05,  ..., -3.1270e-06,\n",
      "         -1.1637e-05, -1.4892e-05],\n",
      "        ...,\n",
      "        [-3.2757e-06,  1.4487e-05,  3.3958e-05,  ..., -1.9008e-05,\n",
      "         -1.1502e-05, -1.6713e-05],\n",
      "        [ 3.1209e-05,  2.4174e-05,  3.1222e-05,  ...,  5.0737e-05,\n",
      "          8.3393e-06, -1.1486e-05],\n",
      "        [ 1.8669e-05, -1.2241e-05, -1.1792e-05,  ..., -5.8957e-05,\n",
      "         -3.5571e-05, -4.1952e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 3.1522e-05, -3.4317e-05,  3.2025e-05,  ..., -6.3055e-05,\n",
      "          4.8655e-05, -7.2296e-05],\n",
      "        [ 6.8952e-05,  1.3404e-04,  1.5423e-05,  ...,  3.9177e-05,\n",
      "          8.3556e-06,  2.6891e-05],\n",
      "        [-5.2330e-05,  7.1140e-05,  8.9655e-05,  ..., -1.2220e-04,\n",
      "         -4.0009e-05, -1.7508e-05],\n",
      "        ...,\n",
      "        [-4.2458e-05,  8.3958e-05,  4.8583e-05,  ...,  1.6983e-05,\n",
      "          2.4305e-05,  3.2969e-05],\n",
      "        [-1.4373e-06, -9.1371e-06,  1.6403e-05,  ..., -2.1813e-05,\n",
      "         -4.9523e-05,  4.6042e-05],\n",
      "        [-5.1923e-05, -3.3124e-05,  1.0985e-05,  ...,  5.3554e-05,\n",
      "          3.6127e-05, -6.8995e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 5.5148e-06, -8.0213e-06,  1.0916e-05,  ...,  1.3967e-05,\n",
      "         -5.2643e-06, -5.1402e-06],\n",
      "        [ 8.2795e-06, -2.6306e-05,  2.3776e-05,  ...,  6.2686e-06,\n",
      "          2.6974e-06, -1.0060e-05],\n",
      "        [ 3.0454e-05,  1.7577e-05,  4.3427e-06,  ...,  1.4435e-05,\n",
      "          8.2628e-06, -1.6365e-06],\n",
      "        ...,\n",
      "        [-2.0220e-05, -1.8026e-05, -1.2259e-05,  ..., -1.2202e-05,\n",
      "          6.1675e-06, -1.4775e-05],\n",
      "        [ 1.4396e-05,  3.1186e-05, -3.8103e-05,  ..., -5.2109e-06,\n",
      "          1.2438e-05, -4.5448e-06],\n",
      "        [-4.3619e-05,  1.1649e-05,  3.1007e-05,  ...,  7.6550e-05,\n",
      "          4.4759e-05,  5.1653e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 2.3642e-05, -2.9843e-05, -6.5833e-05,  ...,  4.5308e-05,\n",
      "         -6.2934e-06,  1.3370e-04],\n",
      "        [-5.1570e-05, -1.7021e-05,  2.9793e-05,  ...,  6.5111e-06,\n",
      "         -4.0575e-05, -5.8733e-05],\n",
      "        [ 1.3550e-04,  6.4186e-05, -2.4615e-04,  ...,  8.2515e-05,\n",
      "          6.9440e-05,  1.1590e-04],\n",
      "        ...,\n",
      "        [ 1.7529e-04, -3.8839e-05, -2.1540e-04,  ...,  2.9000e-05,\n",
      "          1.2570e-05,  6.2507e-05],\n",
      "        [ 2.9155e-05,  3.4134e-05, -1.5286e-04,  ...,  4.3242e-05,\n",
      "          1.7823e-05,  6.0712e-05],\n",
      "        [ 8.8080e-05,  6.5634e-05,  8.8730e-05,  ..., -1.2473e-05,\n",
      "         -6.5925e-05, -1.3169e-04]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-6.4471e-06, -1.1779e-05,  8.1207e-06,  ...,  8.9430e-06,\n",
      "         -2.0809e-05,  4.9632e-05],\n",
      "        [ 2.4501e-05, -1.5738e-05,  3.4319e-06,  ..., -1.6284e-05,\n",
      "          1.2516e-05, -1.1578e-05],\n",
      "        [-2.5705e-05, -2.6684e-05, -3.5003e-05,  ...,  1.8101e-05,\n",
      "          1.7461e-05,  1.7930e-05],\n",
      "        ...,\n",
      "        [-1.3542e-05, -5.2064e-06, -3.7538e-06,  ...,  1.7383e-05,\n",
      "         -3.7291e-06, -1.3454e-05],\n",
      "        [-2.4377e-06, -2.1940e-05,  4.3122e-06,  ..., -5.2929e-06,\n",
      "         -8.5857e-06,  1.0711e-05],\n",
      "        [-1.9436e-05, -3.9420e-06, -7.4758e-07,  ...,  4.6356e-06,\n",
      "          6.0381e-06, -6.3948e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 4.0302e-05,  9.9437e-07,  3.8378e-05,  ...,  9.9052e-06,\n",
      "          6.0441e-05, -5.7731e-05],\n",
      "        [-1.4138e-04,  5.4019e-05, -4.3833e-05,  ..., -2.1951e-05,\n",
      "         -1.0541e-05,  1.5234e-04],\n",
      "        [ 3.1382e-05, -1.1841e-05,  2.9477e-05,  ..., -1.0982e-04,\n",
      "          1.6640e-05, -6.1470e-05],\n",
      "        ...,\n",
      "        [-5.7453e-05,  3.9633e-05,  2.5673e-05,  ..., -4.7237e-05,\n",
      "          6.4037e-06, -1.0608e-04],\n",
      "        [-1.7230e-04,  6.4790e-05, -5.5035e-05,  ..., -5.1868e-05,\n",
      "          3.3865e-05,  1.5579e-04],\n",
      "        [ 1.1997e-04, -1.1287e-04,  5.3195e-05,  ...,  3.6703e-05,\n",
      "          3.2913e-05, -4.9529e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-4.0762e-05, -3.3939e-05,  1.5886e-05,  ..., -8.0659e-06,\n",
      "          8.7554e-06,  5.0868e-06],\n",
      "        [-1.7002e-05, -7.2816e-06,  9.0444e-05,  ...,  6.2784e-06,\n",
      "         -4.0303e-06,  5.6513e-08],\n",
      "        [ 2.3783e-05, -1.7847e-05,  1.5803e-05,  ...,  9.8687e-06,\n",
      "         -2.0932e-05, -1.5279e-05],\n",
      "        ...,\n",
      "        [-2.3430e-05,  5.2362e-06, -5.1347e-06,  ..., -9.0975e-06,\n",
      "         -1.0298e-06, -6.9574e-06],\n",
      "        [ 1.3023e-05, -2.9934e-05,  8.0060e-06,  ...,  7.5857e-06,\n",
      "          2.0668e-05,  3.8444e-06],\n",
      "        [-1.0303e-05,  2.0163e-05, -1.0231e-05,  ...,  5.2481e-06,\n",
      "         -6.2524e-06, -5.8361e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 4.7204e-05,  1.4435e-05, -9.9522e-05,  ..., -1.6178e-05,\n",
      "         -3.1984e-05,  5.5661e-05],\n",
      "        [-3.2862e-05, -1.7490e-04, -2.1046e-04,  ..., -9.6149e-05,\n",
      "          2.2957e-05,  1.2466e-04],\n",
      "        [ 2.0961e-05, -1.6911e-04,  1.2761e-04,  ..., -4.9583e-05,\n",
      "          3.9937e-05,  9.4352e-05],\n",
      "        ...,\n",
      "        [-3.4879e-05,  8.6482e-06, -8.7733e-05,  ...,  9.8426e-06,\n",
      "          1.1060e-05, -2.5707e-05],\n",
      "        [-1.4205e-04, -1.1819e-04, -7.7254e-05,  ..., -8.7853e-05,\n",
      "         -6.2191e-05, -5.6126e-05],\n",
      "        [ 3.6292e-05,  5.8480e-05,  2.3616e-04,  ...,  4.1922e-05,\n",
      "          7.2971e-05,  4.2077e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 6.8290e-06,  5.1833e-05, -1.0471e-05,  ..., -9.8011e-06,\n",
      "          2.2319e-05, -3.3903e-05],\n",
      "        [ 1.9067e-05, -2.8188e-05, -1.8536e-05,  ..., -9.1406e-07,\n",
      "          1.8878e-05, -2.4248e-05],\n",
      "        [-1.7087e-06,  3.1679e-06, -4.5011e-07,  ...,  1.5034e-05,\n",
      "         -8.0515e-07, -3.2016e-05],\n",
      "        ...,\n",
      "        [ 9.1455e-06, -2.7787e-06,  8.6800e-06,  ..., -4.9815e-06,\n",
      "          9.2581e-06,  1.6476e-05],\n",
      "        [ 3.5748e-06, -1.7325e-05, -1.1954e-05,  ...,  1.9637e-05,\n",
      "         -6.7967e-08, -3.6998e-06],\n",
      "        [-8.4556e-06,  1.5149e-05,  1.8170e-05,  ...,  1.8243e-06,\n",
      "          6.8889e-07, -1.2601e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-1.0527e-05,  2.0125e-05,  7.7770e-05,  ...,  6.3824e-05,\n",
      "         -2.0367e-05,  8.9590e-05],\n",
      "        [ 8.5320e-05,  9.5662e-05, -1.2444e-04,  ..., -6.6603e-06,\n",
      "         -3.8652e-05,  4.8800e-05],\n",
      "        [-2.3207e-05,  1.3764e-06, -1.3145e-04,  ..., -9.5373e-05,\n",
      "         -3.0081e-05,  3.7344e-05],\n",
      "        ...,\n",
      "        [-2.4974e-06,  7.6243e-05, -8.1123e-05,  ...,  2.1929e-05,\n",
      "          2.9564e-05,  7.5401e-05],\n",
      "        [-3.1501e-05,  1.8081e-05, -1.2021e-05,  ...,  9.3898e-05,\n",
      "         -1.3895e-04,  2.7892e-06],\n",
      "        [ 3.6211e-05, -4.4480e-05,  4.9659e-05,  ..., -3.0553e-06,\n",
      "          2.4351e-05, -6.1914e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 2.1424e-05,  3.3663e-05,  1.5235e-05,  ..., -2.6449e-05,\n",
      "          2.1491e-05,  6.1808e-06],\n",
      "        [-1.1938e-05,  1.6552e-05, -2.3675e-05,  ...,  4.7855e-05,\n",
      "         -6.2307e-05,  1.3754e-05],\n",
      "        [ 2.5427e-05,  9.0059e-06, -1.0139e-05,  ..., -3.0702e-05,\n",
      "          3.7407e-05, -1.8540e-05],\n",
      "        ...,\n",
      "        [-3.1281e-06, -1.4918e-06, -1.3551e-06,  ..., -1.9628e-05,\n",
      "         -2.7958e-05,  2.2813e-05],\n",
      "        [-6.1230e-05,  2.0130e-05,  1.9760e-05,  ...,  8.4693e-06,\n",
      "          7.2508e-05,  6.9485e-06],\n",
      "        [-1.6108e-05, -7.1994e-06,  2.1534e-05,  ...,  2.7782e-06,\n",
      "         -1.6200e-05, -1.5860e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.1795e-05, -1.5666e-04,  4.2521e-05,  ...,  1.4263e-04,\n",
      "          8.5362e-05, -3.2502e-05],\n",
      "        [-2.4849e-05,  1.3449e-04,  2.3348e-05,  ..., -1.0628e-04,\n",
      "         -9.4362e-05, -5.5417e-05],\n",
      "        [ 1.0943e-04,  4.5307e-06, -1.4326e-04,  ..., -9.0477e-05,\n",
      "         -6.2578e-05, -4.9635e-05],\n",
      "        ...,\n",
      "        [ 1.4512e-05,  1.1746e-04, -5.2444e-05,  ...,  1.0176e-05,\n",
      "          4.0465e-05, -9.6802e-05],\n",
      "        [-4.3997e-05,  6.7062e-05,  2.0460e-05,  ...,  2.6385e-05,\n",
      "          4.4515e-05,  1.6542e-05],\n",
      "        [ 8.4650e-05,  9.2795e-06, -9.8030e-05,  ...,  3.8200e-06,\n",
      "         -8.8057e-06,  3.0886e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-1.2546e-05,  1.6568e-05, -1.9743e-06,  ..., -3.4592e-05,\n",
      "         -1.9285e-05,  8.3094e-06],\n",
      "        [-9.4749e-06,  1.4208e-06,  7.1221e-06,  ...,  1.1367e-05,\n",
      "         -5.7604e-06,  6.9483e-06],\n",
      "        [-3.0539e-05, -5.6113e-06,  1.4620e-05,  ..., -3.0120e-05,\n",
      "          3.7827e-05,  2.2228e-05],\n",
      "        ...,\n",
      "        [-1.8494e-05, -2.1435e-05,  8.9292e-06,  ...,  1.7456e-05,\n",
      "          7.2668e-06,  2.4066e-05],\n",
      "        [ 4.6682e-05, -1.0226e-05, -3.3310e-06,  ..., -1.2824e-07,\n",
      "          2.7989e-05, -1.4623e-05],\n",
      "        [-2.3531e-05,  9.7105e-06,  3.1544e-05,  ...,  1.4202e-06,\n",
      "          6.1936e-05, -2.8994e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.0213e-05,  3.7429e-07,  1.2607e-04,  ..., -6.9763e-06,\n",
      "          6.8269e-05, -3.7907e-05],\n",
      "        [ 5.4708e-05, -7.1517e-05, -2.6021e-05,  ...,  3.1882e-06,\n",
      "          6.3968e-05,  3.9159e-05],\n",
      "        [ 5.8298e-05, -1.6471e-05, -1.1762e-05,  ...,  1.0524e-04,\n",
      "         -4.2868e-05, -1.2539e-05],\n",
      "        ...,\n",
      "        [-9.3779e-05,  8.4423e-05,  6.5422e-05,  ..., -1.0362e-05,\n",
      "         -9.0983e-05,  9.8435e-05],\n",
      "        [ 2.8074e-05, -8.1858e-07, -1.2571e-05,  ..., -8.4270e-06,\n",
      "          3.8673e-05, -5.8830e-06],\n",
      "        [-9.2950e-05,  1.2499e-04,  5.3667e-06,  ...,  3.0466e-06,\n",
      "         -2.8367e-05,  1.6187e-04]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-1.3832e-05, -2.5882e-05, -3.4662e-05,  ...,  5.2517e-05,\n",
      "         -1.9661e-05, -3.5460e-05],\n",
      "        [ 7.3214e-06, -3.1223e-05, -3.3482e-05,  ..., -2.4921e-05,\n",
      "         -6.9450e-06, -3.5295e-06],\n",
      "        [-3.3867e-06, -6.0579e-05, -1.9704e-05,  ..., -2.3218e-05,\n",
      "         -2.4709e-06, -3.6868e-05],\n",
      "        ...,\n",
      "        [-9.0803e-06,  2.8170e-05,  1.6239e-05,  ..., -2.5630e-05,\n",
      "         -5.7070e-06,  1.8341e-06],\n",
      "        [-5.3534e-05, -2.3169e-04,  1.8945e-04,  ...,  4.0110e-05,\n",
      "         -3.3661e-05, -8.9207e-05],\n",
      "        [-2.0243e-05, -7.8127e-06,  1.1176e-05,  ..., -6.3668e-06,\n",
      "         -3.1225e-05,  1.9376e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-1.3225e-05,  3.2476e-05, -8.6330e-05,  ..., -6.0155e-06,\n",
      "         -3.0376e-05,  9.1290e-05],\n",
      "        [ 2.8998e-05, -6.2363e-05,  5.8746e-05,  ...,  6.0110e-06,\n",
      "          3.0535e-05, -7.0141e-05],\n",
      "        [-6.7112e-05, -3.9958e-05, -4.1024e-05,  ..., -2.3836e-05,\n",
      "          6.3668e-06,  2.4244e-05],\n",
      "        ...,\n",
      "        [ 4.6784e-05, -4.7191e-05, -6.7034e-05,  ..., -4.5818e-05,\n",
      "         -3.9098e-05, -6.5552e-06],\n",
      "        [-7.9394e-06, -3.6338e-05, -6.9853e-05,  ..., -1.4416e-04,\n",
      "          2.9826e-05,  1.0019e-05],\n",
      "        [-4.7392e-05, -1.1204e-05,  4.5296e-06,  ..., -3.3600e-05,\n",
      "         -7.4721e-05,  3.5474e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 4.2958e-05, -5.2527e-05, -9.7513e-07,  ...,  1.8657e-06,\n",
      "          3.9931e-05,  1.2120e-05],\n",
      "        [-1.4079e-05,  4.8738e-05, -1.2254e-05,  ...,  1.4678e-05,\n",
      "         -3.0045e-05, -5.0736e-06],\n",
      "        [-7.3421e-06,  8.2985e-05,  3.2791e-05,  ...,  4.6221e-05,\n",
      "          7.5837e-06,  2.2862e-05],\n",
      "        ...,\n",
      "        [ 3.3359e-05, -8.3444e-05, -1.8548e-05,  ..., -5.8008e-05,\n",
      "          5.2226e-05,  5.4791e-05],\n",
      "        [ 3.7797e-06, -1.1908e-05, -5.8051e-05,  ..., -1.3218e-05,\n",
      "         -1.5190e-05,  3.3049e-05],\n",
      "        [ 4.0294e-05, -1.1615e-04, -3.2229e-05,  ..., -9.3541e-05,\n",
      "          5.5428e-05,  1.5353e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-1.6745e-05,  1.2548e-04, -2.9586e-05,  ..., -5.8590e-05,\n",
      "         -2.0413e-05, -4.3749e-05],\n",
      "        [ 1.3810e-05,  6.9991e-05, -8.1958e-05,  ..., -2.4316e-05,\n",
      "          5.6441e-05, -6.3763e-06],\n",
      "        [ 1.9541e-05, -8.8242e-05, -5.8703e-05,  ..., -9.3137e-06,\n",
      "         -6.3494e-06,  5.5903e-05],\n",
      "        ...,\n",
      "        [ 1.7354e-04,  6.6402e-05,  2.0982e-04,  ...,  6.6034e-06,\n",
      "         -1.0740e-04, -1.0631e-04],\n",
      "        [ 1.3757e-05,  1.4345e-05,  6.3703e-05,  ..., -6.8515e-06,\n",
      "         -1.9249e-05, -6.9616e-06],\n",
      "        [-2.7129e-05, -1.0650e-05, -1.6902e-05,  ...,  7.3547e-05,\n",
      "         -2.7933e-05,  1.6622e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 0.0022,  0.0354, -0.0098,  ...,  0.0112,  0.0042, -0.0051],\n",
      "        [ 0.0220, -0.0079,  0.0207,  ...,  0.0194, -0.0022,  0.0127],\n",
      "        [-0.0049,  0.0014, -0.0176,  ..., -0.0114, -0.0077,  0.0088],\n",
      "        [-0.0106, -0.0116,  0.0169,  ..., -0.0171, -0.0174, -0.0057],\n",
      "        [-0.0087, -0.0172, -0.0103,  ..., -0.0020,  0.0232, -0.0106]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([-0.0132,  0.0200, -0.0019,  0.0066, -0.0115], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>))\n"
     ]
    }
   ],
   "source": [
    "import learn2learn as l2l\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "meta_lr = 0.001\n",
    "fast_lr = 0.1\n",
    "maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\n",
    "opt = optim.Adam(maml.parameters(), meta_lr)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "for name, param in maml.named_parameters():\n",
    "    print(f\"Before cloning - {name}: requires_grad={param.requires_grad}, grad_fn={param.grad_fn}\")\n",
    "\n",
    "learner = maml.clone()\n",
    "for name, param in learner.named_parameters():\n",
    "    print(f\"After cloning - {name}: requires_grad={param.requires_grad}, grad_fn={param.grad_fn}\")\n",
    "tx, ty=tasksets.train.sample()\n",
    "tx, ty=tx.to(device), ty.to(device)\n",
    "# gradients did not propagate normally on the replicate\n",
    "loss=loss_fn(learner(tx)[0], ty)\n",
    "diff_param=[p for p in learner.parameters() if p.requires_grad]\n",
    "grad=torch.autograd.grad(loss, diff_param, retain_graph=True, create_graph=True, allow_unused=True)\n",
    "print(\"-------------------\")\n",
    "print(grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker)",
   "language": "python",
   "name": "docker_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
