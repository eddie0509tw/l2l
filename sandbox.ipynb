{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from lightning.pytorch import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from dats import build_dataset\n",
    "from model import build_model\n",
    "# from tools.vis import save_attention_loc\n",
    "from tools.validator import build_validator\n",
    "\n",
    "@hydra.main(config_path=\"conf\", config_name=\"finetune\", version_base=\"1.3\")\n",
    "def run(cfg: DictConfig):\n",
    "    save_dir = (f\"{cfg.dataset.name}_result/{cfg.task.name}/{cfg.model.name}\")\n",
    "    save_path = os.path.join(cfg.root_dir, save_dir)\n",
    "    meta_dataloader = build_dataset(cfg)\n",
    "    train_loader = meta_dataloader['train']\n",
    "    val_loader = meta_dataloader['val']\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    ckpt_cb = ModelCheckpoint(\n",
    "        dirpath=os.path.join(save_path, \"weight\"),\n",
    "        filename=\"best\",\n",
    "        monitor='val/loss',\n",
    "        mode='min',\n",
    "        save_top_k=1,\n",
    "        save_last=True)\n",
    "    callbacks = [lr_monitor, ckpt_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_186454/771495399.py:7: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"conf\")  # Make sure 'conf' is the correct path relative to your notebook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir: datasets\n",
      "batch_size: 8\n",
      "num_workers: 2\n",
      "num_epochs: 10\n",
      "eval_steps: 1000\n",
      "learning_rate: 5.0e-05\n",
      "weight_decay: 0.0001\n",
      "scheduler:\n",
      "  milestones:\n",
      "  - 6\n",
      "  - 8\n",
      "  gamma: 0.1\n",
      "wandb:\n",
      "  project: deberta-glue\n",
      "  entity: eddie880509\n",
      "  tags:\n",
      "  - lora\n",
      "gpu: 0\n",
      "load_pretrained: true\n",
      "dataset:\n",
      "  name: glue\n",
      "  root_dir: datasets\n",
      "  overwrite_cache: false\n",
      "  use_tokenizer: true\n",
      "task:\n",
      "  name: cola\n",
      "  max_seq_length: 64\n",
      "  num_labels: 2\n",
      "  learning_rate: 5.0e-05\n",
      "model:\n",
      "  name: deberta_large\n",
      "  model_name: microsoft/deberta-large\n",
      "  use_hf_weights: true\n",
      "  output_attentions: true\n",
      "  apply_lora: true\n",
      "  lora:\n",
      "    r: 16\n",
      "    lora_alpha: 32\n",
      "    lora_dropout: 0.0\n",
      "    lora_nums: 1\n",
      "    bias: none\n",
      "    target_modules:\n",
      "    - self.in_proj\n",
      "    modules_to_save:\n",
      "    - classifier\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose, initialize\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "# Initialize Hydra and compose the config manually\n",
    "initialize(config_path=\"conf\")  # Make sure 'conf' is the correct path relative to your notebook\n",
    "cfg = compose(config_name=\"finetune\")  # Load the \"train.yaml\" config\n",
    "\n",
    "# Print the default configuration\n",
    "print(OmegaConf.to_yaml(cfg))  # Optional: To see the loaded configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "DebertaForSequenceClassification(\n",
      "  (deberta): DebertaModel(\n",
      "    (embeddings): DebertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n",
      "      (LayerNorm): DebertaLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): DebertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x DebertaLayer(\n",
      "          (attention): DebertaAttention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): DebertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): DebertaLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): DebertaLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(1024, 1024)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import build_model\n",
    "model = build_model(cfg)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_meta_config {'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.0, 'lora_nums': 1, 'bias': 'none', 'target_modules': ['self.in_proj'], 'modules_to_save': ['classifier']}\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "lora_meta_config = cfg.model.get(\"lora\", None)\n",
    "print(\"lora_meta_config\", lora_meta_config)\n",
    "lora_config = LoraConfig(\n",
    "        **lora_meta_config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dataloader = build_dataset(cfg)\n",
    "train_loader = meta_dataloader['train']\n",
    "val_loader = meta_dataloader['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validator <tools.validator.GlueValidator object at 0x7f86fc8dbdc0>\n"
     ]
    }
   ],
   "source": [
    "from tools.validator import build_validator\n",
    "validator = build_validator(cfg)\n",
    "print(\"validator\", validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels tensor([1, 1, 1, 1, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "loss, logits, all_hidden_states, all_attentions = model(**batch)\n",
    "preds = logits.detach()\n",
    "labels = batch['labels']\n",
    "print(\"labels\", labels)\n",
    "results = validator.get_metrics(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "ways = 5\n",
    "model_name='google/vit-base-patch16-224-in21k'\n",
    "config = ViTConfig.from_pretrained(model_name, num_labels=ways)\n",
    "model = ViTForImageClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): ViTForImageClassification(\n",
      "      (vit): ViTModel(\n",
      "        (embeddings): ViTEmbeddings(\n",
      "          (patch_embeddings): ViTPatchEmbeddings(\n",
      "            (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (encoder): ViTEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x ViTLayer(\n",
      "              (attention): ViTSdpaAttention(\n",
      "                (attention): ViTSdpaSelfAttention(\n",
      "                  (query): Linear(\n",
      "                    in_features=768, out_features=768, bias=True\n",
      "                    (lora_route): Linear(in_features=768, out_features=1, bias=False)\n",
      "                    (lora_A0): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    (lora_B0): Linear(in_features=16, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(\n",
      "                    in_features=768, out_features=768, bias=True\n",
      "                    (lora_route): Linear(in_features=768, out_features=1, bias=False)\n",
      "                    (lora_A0): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    (lora_B0): Linear(in_features=16, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): ViTSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): ViTIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): ViTOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"query\",\"value\"],\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        modules_to_save=[\"classifier\"],\n",
    "        lora_nums=1,\n",
    "    )\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dats.dataset import create_miniimgnat, create_omniglot\n",
    "ways = 5\n",
    "shots = 5\n",
    "tasksets = create_miniimgnat(train_samples=2*shots,\n",
    "                                    train_ways=ways,\n",
    "                                    test_samples=2*shots,\n",
    "                                    test_ways=ways,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.vit.encoder.layer.0.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.query.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.query.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.query.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.value.lora_route.weight torch.Size([1, 768])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.value.lora_A0.weight torch.Size([16, 768])\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.value.lora_B0.weight torch.Size([768, 16])\n",
      "base_model.model.classifier.weight torch.Size([5, 768])\n",
      "base_model.model.classifier.bias torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ViTForImageClassification(\n",
       "      (vit): ViTModel(\n",
       "        (embeddings): ViTEmbeddings(\n",
       "          (patch_embeddings): ViTPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): ViTEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x ViTLayer(\n",
       "              (attention): ViTSdpaAttention(\n",
       "                (attention): ViTSdpaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=768, out_features=768, bias=True\n",
       "                    (lora_route): Linear(in_features=768, out_features=1, bias=False)\n",
       "                    (lora_A0): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    (lora_B0): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=768, out_features=768, bias=True\n",
       "                    (lora_route): Linear(in_features=768, out_features=1, bias=False)\n",
       "                    (lora_A0): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    (lora_B0): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): ViTSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): ViTIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): ViTOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "cuda = torch.cuda.is_available()\n",
    "seed = 42\n",
    "if cuda and torch.cuda.device_count():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cloning - module.base_model.model.vit.embeddings.cls_token: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.embeddings.position_embeddings: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.embeddings.patch_embeddings.projection.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.embeddings.patch_embeddings.projection.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.0.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.1.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.2.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.3.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.4.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.5.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.6.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.7.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.8.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.9.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.10.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.encoder.layer.11.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.layernorm.weight: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.vit.layernorm.bias: requires_grad=False, grad_fn=None\n",
      "Before cloning - module.base_model.model.classifier.weight: requires_grad=True, grad_fn=None\n",
      "Before cloning - module.base_model.model.classifier.bias: requires_grad=True, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.embeddings.cls_token: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.embeddings.position_embeddings: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.embeddings.patch_embeddings.projection.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.embeddings.patch_embeddings.projection.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.0.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.1.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.2.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.3.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.4.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.5.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.6.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.7.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.8.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.9.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.10.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.query.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.key.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.key.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_route.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_A0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f54610>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.attention.value.lora_B0.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.attention.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.intermediate.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.intermediate.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.output.dense.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.output.dense.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.layernorm_before.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.layernorm_before.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.layernorm_after.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.encoder.layer.11.layernorm_after.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.layernorm.weight: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.vit.layernorm.bias: requires_grad=False, grad_fn=None\n",
      "After cloning - module.base_model.model.classifier.weight: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "After cloning - module.base_model.model.classifier.bias: requires_grad=True, grad_fn=<CloneBackward0 object at 0x7f60d4f3ec50>\n",
      "-------------------\n",
      "(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.4712e-06, -2.5180e-07, -3.4850e-06,  ...,  7.2785e-06,\n",
      "          1.4621e-06, -6.8089e-07],\n",
      "        [-1.2693e-06, -9.7646e-07,  9.2527e-06,  ..., -1.0092e-07,\n",
      "          2.6564e-06,  8.8900e-07],\n",
      "        [ 8.0796e-06, -2.5109e-05,  2.2002e-06,  ..., -2.1584e-05,\n",
      "         -1.3952e-05,  1.5982e-06],\n",
      "        ...,\n",
      "        [ 1.6275e-06,  5.7283e-06,  1.0749e-05,  ...,  5.4056e-06,\n",
      "         -2.6686e-06, -1.6559e-06],\n",
      "        [-3.1960e-06,  1.0137e-05,  4.5186e-06,  ...,  1.0806e-05,\n",
      "          9.1700e-06,  4.8159e-06],\n",
      "        [ 1.4507e-06,  6.4119e-06,  6.5274e-06,  ...,  7.0665e-06,\n",
      "          1.9924e-06,  2.1559e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-3.2510e-05, -3.7646e-05, -9.7353e-05,  ...,  3.1152e-05,\n",
      "         -1.5500e-05, -1.0710e-05],\n",
      "        [-1.0881e-04, -8.5594e-06, -6.4567e-06,  ..., -6.4998e-05,\n",
      "         -1.2070e-04,  1.9148e-05],\n",
      "        [ 5.1500e-06,  7.5972e-05,  7.7882e-05,  ...,  8.9217e-06,\n",
      "         -2.9186e-05, -2.1628e-05],\n",
      "        ...,\n",
      "        [ 7.7805e-06, -4.0149e-06,  2.8747e-06,  ...,  2.7685e-06,\n",
      "          1.3854e-05, -2.6554e-05],\n",
      "        [ 1.4118e-05, -1.4922e-05, -7.2413e-06,  ..., -3.2031e-06,\n",
      "          7.5815e-06, -2.4332e-05],\n",
      "        [-8.7408e-06,  8.4546e-06,  1.6716e-06,  ..., -1.2594e-05,\n",
      "         -7.5562e-06,  2.3396e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 2.2364e-05, -5.0161e-06,  1.5986e-05,  ...,  6.2238e-06,\n",
      "         -1.7164e-05, -2.3366e-05],\n",
      "        [ 6.8315e-06,  8.8768e-06,  1.2826e-05,  ...,  2.9705e-06,\n",
      "         -1.5143e-06, -1.2254e-05],\n",
      "        [-1.2255e-05,  1.0746e-05,  1.2278e-05,  ..., -7.5345e-06,\n",
      "         -1.5697e-05, -7.1552e-06],\n",
      "        ...,\n",
      "        [ 3.1598e-05, -7.9795e-06,  3.7520e-06,  ...,  1.9381e-05,\n",
      "         -2.9134e-06, -1.0998e-05],\n",
      "        [ 7.2749e-06, -1.8324e-06, -2.3008e-06,  ...,  5.2132e-06,\n",
      "         -1.8071e-06, -5.6828e-06],\n",
      "        [ 1.2893e-05, -9.9270e-07,  1.9288e-05,  ..., -1.5823e-06,\n",
      "         -1.5492e-05, -2.7583e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-3.7508e-05,  2.5451e-06,  9.8079e-05,  ...,  2.8146e-05,\n",
      "         -5.7358e-05, -4.2136e-05],\n",
      "        [-2.3079e-05, -5.4972e-06,  3.8204e-05,  ...,  1.8488e-05,\n",
      "         -6.5560e-05,  1.4202e-06],\n",
      "        [ 1.1236e-04,  4.0215e-05, -6.0323e-05,  ..., -1.6084e-05,\n",
      "          3.6563e-05,  1.0705e-04],\n",
      "        ...,\n",
      "        [-7.1564e-05,  7.0428e-06, -3.6067e-06,  ..., -1.3873e-05,\n",
      "         -5.9228e-05, -4.9120e-05],\n",
      "        [-1.6058e-04,  5.3192e-05,  5.2342e-05,  ...,  9.5765e-05,\n",
      "         -9.0020e-05, -5.4887e-05],\n",
      "        [-7.5793e-05, -4.9473e-05, -8.6575e-05,  ..., -2.1574e-05,\n",
      "          1.0589e-04,  3.3180e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-7.4261e-06, -2.0928e-06,  6.4378e-06,  ...,  2.0961e-05,\n",
      "          6.6383e-06,  1.0042e-05],\n",
      "        [ 2.7208e-05,  8.1989e-06,  1.1311e-05,  ...,  1.3056e-05,\n",
      "         -2.6218e-05, -9.8966e-06],\n",
      "        [ 3.1164e-05,  1.8252e-05,  2.1555e-06,  ..., -6.4028e-06,\n",
      "         -9.5341e-06, -7.6790e-06],\n",
      "        ...,\n",
      "        [ 1.9221e-05,  2.6586e-05, -4.1582e-05,  ..., -4.9212e-05,\n",
      "          4.0457e-05,  3.3394e-05],\n",
      "        [ 2.6789e-05,  5.6357e-05, -4.5454e-05,  ...,  1.8633e-05,\n",
      "         -1.4584e-05,  5.3269e-06],\n",
      "        [-5.0555e-05,  2.1691e-05,  1.1574e-05,  ..., -2.3461e-05,\n",
      "          2.3997e-05, -1.2395e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-1.5581e-04, -6.4159e-05, -5.2599e-05,  ..., -1.0467e-04,\n",
      "          4.0877e-05, -6.3353e-05],\n",
      "        [ 2.2121e-05,  3.3685e-05,  2.5662e-05,  ...,  1.3040e-04,\n",
      "         -5.9229e-05,  8.2455e-05],\n",
      "        [-2.6725e-05,  1.1388e-05, -1.0355e-04,  ..., -6.4509e-05,\n",
      "          2.1523e-05,  3.9337e-05],\n",
      "        ...,\n",
      "        [ 1.6405e-04,  1.4585e-04, -3.8514e-05,  ...,  1.1193e-04,\n",
      "         -2.4013e-04,  1.3968e-05],\n",
      "        [ 2.6633e-04,  2.6976e-04,  2.5014e-05,  ...,  1.4086e-04,\n",
      "         -2.7010e-04,  3.0231e-06],\n",
      "        [-8.6964e-06,  2.3326e-06, -3.4369e-05,  ..., -1.1071e-04,\n",
      "          1.4192e-05, -5.6600e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-2.1523e-06, -1.4964e-05,  1.3987e-05,  ...,  9.6224e-06,\n",
      "         -2.3636e-05,  7.6687e-06],\n",
      "        [-1.6711e-06,  2.4553e-05, -1.7154e-05,  ...,  1.2410e-06,\n",
      "         -1.4486e-05, -1.4829e-05],\n",
      "        [ 2.4297e-06, -2.6786e-06,  1.0611e-05,  ..., -1.7333e-06,\n",
      "          2.9407e-05, -1.7801e-05],\n",
      "        ...,\n",
      "        [-8.3055e-06,  2.0834e-05,  1.3056e-05,  ..., -2.3592e-05,\n",
      "         -7.9271e-06,  7.8659e-06],\n",
      "        [ 1.4523e-05,  8.9249e-06,  1.6262e-05,  ...,  2.5792e-05,\n",
      "          4.3289e-06, -6.4109e-06],\n",
      "        [-2.7184e-05,  1.2389e-05,  1.0506e-05,  ..., -2.3714e-05,\n",
      "         -7.4298e-06, -9.0996e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-7.5832e-05,  2.2790e-05,  5.2417e-05,  ...,  8.1395e-05,\n",
      "          9.2673e-06,  2.5850e-05],\n",
      "        [-7.7387e-05, -5.8976e-06,  7.5815e-05,  ..., -1.2296e-05,\n",
      "          3.0774e-05, -5.7002e-05],\n",
      "        [-1.8523e-04,  2.2281e-06,  1.8727e-05,  ..., -2.2023e-05,\n",
      "          5.9106e-05,  1.1215e-05],\n",
      "        ...,\n",
      "        [ 8.9664e-05,  4.3023e-05, -4.8480e-06,  ..., -3.3962e-05,\n",
      "          2.9847e-05, -6.5442e-05],\n",
      "        [ 4.3300e-05, -1.6496e-05,  4.4470e-05,  ...,  2.0825e-06,\n",
      "         -6.9273e-05, -7.5114e-05],\n",
      "        [-4.2931e-05,  8.5090e-05,  1.1714e-04,  ..., -8.7021e-05,\n",
      "         -3.8157e-05, -5.4360e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 3.3774e-05, -5.0346e-06,  1.6987e-05,  ...,  2.7699e-06,\n",
      "         -1.1080e-05, -3.0198e-07],\n",
      "        [ 1.4849e-05,  1.5942e-05,  2.0505e-05,  ...,  8.4083e-06,\n",
      "          1.1459e-05, -1.0722e-05],\n",
      "        [-1.6078e-05,  5.2734e-06,  7.5839e-06,  ..., -1.8097e-05,\n",
      "         -1.7918e-07, -9.3456e-06],\n",
      "        ...,\n",
      "        [-2.4078e-05,  2.8457e-05, -3.0741e-06,  ...,  3.8856e-05,\n",
      "          1.9164e-06, -2.5550e-05],\n",
      "        [ 1.0570e-05,  7.6574e-06,  3.4639e-05,  ..., -2.3074e-05,\n",
      "          4.4913e-05, -1.6758e-05],\n",
      "        [ 3.4391e-06,  1.7341e-06,  7.1278e-06,  ...,  1.3473e-05,\n",
      "         -1.4049e-05,  3.3564e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-9.3059e-05, -3.1994e-05,  6.2878e-05,  ...,  4.2308e-05,\n",
      "         -2.3014e-05,  7.6315e-07],\n",
      "        [-1.1043e-04, -1.5261e-05, -3.5196e-05,  ...,  6.4985e-05,\n",
      "          4.0381e-05, -3.2732e-05],\n",
      "        [ 1.2394e-04, -6.1145e-05,  1.5517e-04,  ...,  5.7487e-06,\n",
      "         -8.3858e-05,  6.3173e-05],\n",
      "        ...,\n",
      "        [-1.2378e-04,  1.5676e-05,  1.0285e-04,  ...,  8.7701e-05,\n",
      "          4.1877e-05, -6.3717e-05],\n",
      "        [-3.8874e-05, -9.6254e-05,  4.3785e-05,  ..., -7.4798e-05,\n",
      "         -8.6073e-06,  5.9148e-05],\n",
      "        [-1.1049e-04,  5.0436e-05, -1.0745e-04,  ...,  4.0065e-05,\n",
      "          7.6749e-05, -6.4702e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-2.3579e-05,  3.1457e-05,  2.7653e-05,  ...,  3.5088e-05,\n",
      "         -4.1454e-05, -2.5988e-05],\n",
      "        [ 5.1335e-06, -2.3064e-05,  6.7067e-06,  ...,  1.6819e-05,\n",
      "         -2.1231e-05, -5.5645e-06],\n",
      "        [ 2.3536e-05, -9.6294e-06, -2.7785e-05,  ..., -2.8493e-05,\n",
      "          4.6048e-05,  7.5827e-06],\n",
      "        ...,\n",
      "        [-3.7618e-06, -9.1536e-06, -7.1831e-06,  ..., -5.8983e-06,\n",
      "          3.3254e-05,  2.2231e-05],\n",
      "        [ 2.5497e-05, -1.0083e-05,  8.1878e-06,  ...,  2.1139e-05,\n",
      "         -7.8439e-06, -9.0753e-06],\n",
      "        [-1.8934e-05,  3.4916e-06, -5.3265e-06,  ...,  1.2191e-05,\n",
      "          6.8686e-06,  6.4672e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-6.6010e-05,  4.5160e-06,  1.8091e-04,  ...,  2.5222e-05,\n",
      "         -2.0835e-05,  1.1319e-04],\n",
      "        [-7.3561e-05, -1.2323e-04,  1.2365e-04,  ..., -1.3050e-05,\n",
      "          1.7760e-04,  1.2507e-04],\n",
      "        [ 9.9869e-05,  2.4939e-05, -3.1056e-06,  ..., -8.5999e-05,\n",
      "         -7.2962e-05, -7.9809e-05],\n",
      "        ...,\n",
      "        [-2.2101e-05,  8.1070e-05,  6.0819e-05,  ..., -3.8936e-05,\n",
      "         -4.0990e-05,  5.5102e-05],\n",
      "        [ 3.6574e-05,  1.7778e-04,  7.6896e-05,  ...,  8.9653e-05,\n",
      "         -1.1960e-05, -6.3788e-05],\n",
      "        [ 7.1323e-05, -9.8588e-05, -3.2701e-05,  ...,  8.8883e-05,\n",
      "          8.8324e-05, -9.2717e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-3.0679e-05,  4.4936e-06,  8.9376e-06,  ...,  9.9167e-07,\n",
      "          8.9755e-06,  2.3807e-05],\n",
      "        [ 5.8192e-06, -9.9957e-06,  1.3936e-05,  ...,  3.7743e-05,\n",
      "          2.7832e-05, -1.7673e-05],\n",
      "        [-2.0010e-05, -1.9977e-05, -2.8461e-05,  ..., -1.6990e-05,\n",
      "         -1.2841e-05,  3.4086e-05],\n",
      "        ...,\n",
      "        [ 1.8309e-05, -2.4247e-06,  1.9186e-05,  ...,  1.7306e-05,\n",
      "          1.1083e-07,  5.4857e-06],\n",
      "        [-7.3411e-05,  7.4262e-05,  5.6543e-05,  ...,  2.6753e-05,\n",
      "          1.8588e-05,  2.4814e-05],\n",
      "        [-1.1578e-05, -4.6846e-05,  5.5891e-05,  ..., -4.4576e-06,\n",
      "         -5.0222e-05,  2.3638e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-3.6741e-05,  3.7188e-04, -1.4399e-04,  ...,  7.6799e-05,\n",
      "          1.2762e-04,  2.3362e-05],\n",
      "        [-2.3060e-05,  8.6702e-05,  1.4448e-04,  ..., -9.6843e-06,\n",
      "          7.9791e-05, -8.1297e-06],\n",
      "        [-5.6991e-05,  1.2891e-04, -4.8386e-06,  ...,  8.8778e-05,\n",
      "          5.5300e-05,  4.0332e-05],\n",
      "        ...,\n",
      "        [ 3.0717e-05, -2.0178e-04, -7.8787e-05,  ..., -2.0491e-05,\n",
      "         -4.1406e-05,  1.5522e-06],\n",
      "        [ 3.0569e-05, -4.1995e-05,  2.4191e-06,  ..., -4.8707e-05,\n",
      "         -7.5371e-05,  5.8120e-05],\n",
      "        [-7.7178e-05, -8.5776e-05,  7.8138e-05,  ..., -1.7260e-05,\n",
      "         -3.3328e-05,  1.6183e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 2.5133e-05, -2.6782e-05,  1.7069e-05,  ..., -2.0095e-07,\n",
      "          1.8748e-05,  1.2474e-05],\n",
      "        [ 3.5209e-05, -6.7490e-07,  2.1936e-05,  ...,  4.0188e-06,\n",
      "          1.1260e-05,  4.0506e-06],\n",
      "        [-2.3834e-05,  2.1988e-06, -3.9968e-05,  ..., -5.5138e-06,\n",
      "          1.7667e-05,  5.5633e-05],\n",
      "        ...,\n",
      "        [-3.4236e-06,  1.3807e-05, -3.2481e-06,  ...,  1.3706e-05,\n",
      "         -1.2526e-05,  1.5938e-05],\n",
      "        [-2.4207e-06,  1.8180e-05, -4.9451e-06,  ..., -4.3371e-06,\n",
      "         -5.1914e-07, -9.4674e-06],\n",
      "        [ 8.2365e-06, -7.3392e-06,  2.6915e-05,  ..., -3.0169e-06,\n",
      "          2.2959e-05,  2.2353e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-5.4676e-06,  3.2775e-05,  2.3206e-05,  ...,  7.1665e-05,\n",
      "         -1.6517e-04,  4.5037e-05],\n",
      "        [ 4.0934e-05,  2.4329e-06,  4.4545e-06,  ..., -1.4524e-04,\n",
      "         -9.3841e-06, -7.7774e-05],\n",
      "        [ 4.6238e-05, -1.3758e-05,  6.3578e-05,  ...,  4.2438e-05,\n",
      "         -4.9787e-05, -4.2441e-05],\n",
      "        ...,\n",
      "        [-9.5849e-05,  2.9544e-06,  1.0324e-04,  ...,  3.5560e-06,\n",
      "         -1.0764e-04,  1.3197e-04],\n",
      "        [-1.1511e-04,  8.9653e-05,  3.7471e-05,  ...,  3.5974e-05,\n",
      "         -7.0024e-05,  1.3538e-05],\n",
      "        [-6.5024e-05,  2.7338e-05,  9.8301e-05,  ...,  9.4403e-05,\n",
      "          7.6845e-06,  5.9291e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[-2.1549e-06,  2.1460e-05,  1.7186e-05,  ...,  2.1220e-05,\n",
      "         -3.7293e-05, -2.9635e-05],\n",
      "        [ 2.2085e-06, -7.6485e-06,  7.1453e-07,  ..., -1.6205e-07,\n",
      "         -2.0663e-05,  5.7787e-06],\n",
      "        [ 4.2099e-05,  6.9229e-05,  1.6219e-05,  ...,  1.4579e-05,\n",
      "         -1.5598e-05, -5.0599e-06],\n",
      "        ...,\n",
      "        [-2.7516e-06, -1.5575e-05,  6.8951e-06,  ...,  6.1702e-06,\n",
      "         -5.5437e-06, -1.2475e-05],\n",
      "        [-6.4294e-06, -2.6998e-06, -7.8262e-05,  ...,  6.3924e-06,\n",
      "          2.4353e-05,  3.9549e-05],\n",
      "        [ 2.3935e-06,  6.1745e-06,  2.8206e-05,  ..., -9.8392e-06,\n",
      "          3.2171e-05,  9.7557e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 5.1695e-05,  1.3777e-04,  3.3033e-05,  ...,  7.2129e-05,\n",
      "         -1.3099e-05,  8.0156e-07],\n",
      "        [-5.1335e-05, -4.9322e-05, -1.1496e-04,  ..., -2.8540e-05,\n",
      "         -1.8392e-05, -5.5901e-05],\n",
      "        [ 4.8584e-05, -3.1404e-05,  6.4511e-05,  ...,  1.4859e-05,\n",
      "          1.5258e-05,  1.0844e-05],\n",
      "        ...,\n",
      "        [-9.4882e-06, -5.0034e-05,  2.7104e-05,  ...,  4.0111e-05,\n",
      "         -9.8657e-05,  5.9956e-05],\n",
      "        [ 9.9423e-06, -1.2641e-04,  3.7437e-05,  ..., -1.1312e-04,\n",
      "         -4.9467e-05,  1.1914e-05],\n",
      "        [ 3.8063e-05, -1.6218e-05,  9.4391e-05,  ...,  2.3040e-05,\n",
      "          9.0186e-05, -4.5007e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 7.3795e-06,  2.4422e-05, -1.9658e-05,  ..., -5.1872e-06,\n",
      "         -1.9831e-05,  7.6009e-06],\n",
      "        [ 5.1808e-06,  2.9529e-05, -2.3046e-05,  ...,  1.4488e-05,\n",
      "         -5.4151e-06,  5.8844e-06],\n",
      "        [ 8.2498e-06,  6.5992e-06, -8.3615e-06,  ..., -7.5468e-06,\n",
      "         -2.1335e-06,  3.4486e-05],\n",
      "        ...,\n",
      "        [-8.5512e-06, -2.0310e-05,  8.0734e-07,  ...,  2.3745e-06,\n",
      "          1.1589e-05, -9.2883e-06],\n",
      "        [-2.3797e-06,  9.5552e-06,  1.9519e-05,  ...,  1.2661e-05,\n",
      "          1.9183e-05, -3.5569e-05],\n",
      "        [ 1.0959e-05,  5.8559e-06, -1.2944e-05,  ...,  1.5990e-05,\n",
      "          1.5340e-05, -2.3772e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.1006e-04, -4.9703e-05, -6.4271e-05,  ..., -8.5844e-05,\n",
      "          5.5175e-06, -2.8738e-05],\n",
      "        [ 1.1710e-06,  1.6450e-07,  4.6106e-05,  ..., -3.8411e-06,\n",
      "          3.0796e-05, -8.0528e-05],\n",
      "        [ 7.0647e-05,  5.9272e-05, -6.2129e-05,  ..., -7.6584e-05,\n",
      "         -6.5356e-05,  6.8915e-06],\n",
      "        ...,\n",
      "        [ 2.8878e-05,  4.2425e-05, -5.2775e-05,  ...,  1.3310e-04,\n",
      "         -6.0553e-05, -5.7071e-06],\n",
      "        [ 5.2421e-05,  2.5066e-05, -5.7527e-05,  ...,  2.2564e-05,\n",
      "         -7.6247e-05,  7.2434e-05],\n",
      "        [ 4.5548e-05,  1.1059e-05, -3.2947e-06,  ...,  1.0234e-05,\n",
      "         -3.3257e-06,  1.0589e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.9398e-05,  3.4883e-05,  3.9562e-05,  ..., -6.3051e-05,\n",
      "          5.1255e-06, -3.3321e-05],\n",
      "        [ 3.3643e-06, -4.2689e-05, -3.0693e-05,  ...,  1.7062e-06,\n",
      "          1.2486e-05, -2.3450e-05],\n",
      "        [-1.3009e-05,  2.6790e-05,  4.5749e-05,  ..., -2.9911e-05,\n",
      "         -2.3817e-05, -2.8354e-05],\n",
      "        ...,\n",
      "        [ 7.1719e-05,  1.6753e-05, -4.4861e-05,  ..., -7.8471e-06,\n",
      "          2.4764e-05, -8.4005e-06],\n",
      "        [ 5.0246e-05, -1.5685e-04,  7.3206e-05,  ..., -2.0926e-05,\n",
      "         -5.3708e-06, -6.1789e-06],\n",
      "        [ 3.1043e-05, -1.2276e-05, -3.4721e-05,  ...,  3.1163e-05,\n",
      "          2.3972e-05, -4.4014e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 4.1868e-05, -3.6921e-05,  2.1654e-05,  ...,  5.5306e-05,\n",
      "          1.6533e-05, -3.6329e-05],\n",
      "        [-5.1742e-05,  5.1015e-05, -9.4700e-05,  ..., -9.1375e-05,\n",
      "          9.7851e-05,  7.9318e-05],\n",
      "        [-7.6891e-05,  4.9896e-05, -8.7550e-05,  ..., -4.7360e-05,\n",
      "         -3.6136e-05,  2.7245e-05],\n",
      "        ...,\n",
      "        [-8.0898e-05, -9.9270e-05, -9.5023e-05,  ...,  6.3181e-05,\n",
      "         -8.1734e-05, -7.6695e-05],\n",
      "        [-3.6947e-06, -6.8159e-07,  4.2294e-05,  ...,  3.0793e-05,\n",
      "          5.7293e-05, -1.0419e-05],\n",
      "        [ 1.9960e-05,  2.1713e-05, -4.3602e-05,  ..., -3.1108e-05,\n",
      "          8.8969e-06, -4.2859e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.3387e-05,  1.2225e-05,  9.3414e-05,  ..., -8.4737e-05,\n",
      "          2.5512e-05, -5.9055e-06],\n",
      "        [-3.2164e-05, -5.8092e-05,  5.2533e-05,  ..., -1.1347e-04,\n",
      "          1.6962e-05,  4.5821e-06],\n",
      "        [ 5.3159e-05, -7.6297e-06, -9.3943e-06,  ..., -1.6648e-05,\n",
      "         -6.5962e-06, -4.3121e-05],\n",
      "        ...,\n",
      "        [-1.6549e-05, -1.4671e-05,  5.9398e-05,  ..., -9.6120e-05,\n",
      "          4.8115e-05,  2.5526e-06],\n",
      "        [-2.5235e-05, -3.1854e-06,  3.6424e-06,  ..., -2.6407e-05,\n",
      "         -1.1748e-05,  2.0680e-05],\n",
      "        [-6.0632e-05, -7.2246e-06,  2.9817e-05,  ..., -8.6590e-05,\n",
      "          4.4098e-06,  5.2563e-06]], device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 7.6526e-05,  3.4743e-05, -5.6066e-06,  ...,  3.8198e-05,\n",
      "         -7.5164e-06, -8.4191e-06],\n",
      "        [-1.7923e-04, -1.5090e-04,  1.1511e-04,  ..., -9.6059e-05,\n",
      "          5.1253e-05, -3.7353e-05],\n",
      "        [-2.1785e-04, -4.8959e-05,  9.3611e-05,  ..., -1.5466e-04,\n",
      "          1.5151e-04,  4.0939e-05],\n",
      "        ...,\n",
      "        [ 8.7272e-06, -3.2597e-05, -1.3963e-05,  ...,  2.0450e-05,\n",
      "         -5.5877e-05, -1.0969e-04],\n",
      "        [ 2.3764e-05,  2.9686e-05,  1.7533e-05,  ...,  1.3427e-05,\n",
      "          4.5988e-05, -1.5096e-06],\n",
      "        [-3.1486e-05, -1.3352e-05,  7.4975e-06,  ..., -9.7415e-05,\n",
      "          4.9059e-05, -2.3461e-05]], device='cuda:0', grad_fn=<TBackward0>), tensor([[ 1.7074e-02,  1.1051e-02, -7.6690e-03,  ...,  1.0141e-02,\n",
      "          1.0722e-03, -7.9236e-03],\n",
      "        [-1.7179e-03, -7.6530e-03,  2.8993e-03,  ...,  3.8942e-03,\n",
      "          3.2303e-04, -1.4786e-02],\n",
      "        [-9.8719e-03, -5.9226e-04,  2.4765e-03,  ...,  4.0664e-05,\n",
      "         -2.0694e-02, -1.4535e-03],\n",
      "        [-1.5403e-02, -8.4836e-03,  9.9001e-04,  ..., -1.6765e-02,\n",
      "         -4.1361e-03,  1.2587e-02],\n",
      "        [ 9.9195e-03,  5.6777e-03,  1.3032e-03,  ...,  2.6899e-03,\n",
      "          2.3435e-02,  1.1576e-02]], device='cuda:0', grad_fn=<TBackward0>), tensor([-0.0238,  0.0047,  0.0081,  0.0175, -0.0065], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>))\n"
     ]
    }
   ],
   "source": [
    "import learn2learn as l2l\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "meta_lr = 0.001\n",
    "fast_lr = 0.1\n",
    "maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\n",
    "opt = optim.Adam(maml.parameters(), meta_lr)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "for name, param in maml.named_parameters():\n",
    "    print(f\"Before cloning - {name}: requires_grad={param.requires_grad}, grad_fn={param.grad_fn}\")\n",
    "\n",
    "learner = maml.clone()\n",
    "for name, param in learner.named_parameters():\n",
    "    print(f\"After cloning - {name}: requires_grad={param.requires_grad}, grad_fn={param.grad_fn}\")\n",
    "tx, ty=tasksets.train.sample()\n",
    "tx, ty=tx.to(device), ty.to(device)\n",
    "# gradients did not propagate normally on the replicate\n",
    "loss=loss_fn(learner(tx)[0], ty)\n",
    "diff_param=[p for p in learner.parameters() if p.requires_grad]\n",
    "grad=torch.autograd.grad(loss, diff_param, retain_graph=True, create_graph=True, allow_unused=True)\n",
    "print(\"-------------------\")\n",
    "print(grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker)",
   "language": "python",
   "name": "docker_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
